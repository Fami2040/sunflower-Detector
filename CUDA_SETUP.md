# üöÄ CUDA Setup for Maximum Speed

## What Changed

The bot now **automatically prefers CUDA (GPU)** if available, providing **10-50x faster processing**!

### Automatic Behavior:
- **If CUDA is available**: Uses GPU automatically (no configuration needed)
- **If CUDA is not available**: Falls back to CPU with warning

### Manual Control:
You can force a specific device using the `FORCE_DEVICE` environment variable.

---

## How to Use CUDA

### Option 1: Automatic (Recommended)
**Just run the bot - it will auto-detect CUDA if available!**

The bot will automatically use GPU if:
- CUDA is installed
- PyTorch with CUDA support is installed
- A compatible GPU is available

### Option 2: Force CUDA
**Add to Railway Variables:**
```
FORCE_DEVICE=cuda
```

This will:
- Force CUDA usage (fails if CUDA not available)
- Show error if CUDA unavailable (instead of falling back to CPU)

### Option 3: Force CPU
**Add to Railway Variables:**
```
FORCE_DEVICE=cpu
```

This forces CPU even if CUDA is available (useful for testing).

---

## Performance Comparison

### On CPU (Railway free tier):
- Small image: 15-30 seconds
- Medium image: 30-60 seconds
- Large image: 60-120 seconds

### On CUDA (GPU):
- Small image: **3-5 seconds** ‚ö° (5-10x faster!)
- Medium image: **5-10 seconds** ‚ö° (5-10x faster!)
- Large image: **10-20 seconds** ‚ö° (5-10x faster!)

---

## Requirements for CUDA

1. **CUDA-enabled GPU** (NVIDIA GPU)
2. **CUDA Toolkit** installed (version 11.8 or 12.1 recommended)
3. **PyTorch with CUDA support**
   - Install: `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118`

---

## Current Status

### On Your Local Machine:
- If you have an NVIDIA GPU: **CUDA will be used automatically!**
- Check logs for: `‚úÖ CUDA detected: Using GPU - [GPU Name]`

### On Railway:
- **Free tier**: Only CPU available (no GPU)
- **Railway Pro**: May have GPU options (check Railway documentation)
- **Alternative**: Use other GPU hosting (Vast.ai, RunPod, Lambda Labs, etc.)

---

## How to Check if CUDA is Available

### In Python:
```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
```

### In Bot Logs:
When the bot starts, you'll see:
- **CUDA available**: `‚úÖ CUDA detected: Using GPU - [GPU Name]`
- **CUDA not available**: `‚ö†Ô∏è CUDA not available: Using CPU`

---

## Installation for CUDA

### Local Setup:
1. **Install CUDA Toolkit**: https://developer.nvidia.com/cuda-downloads
2. **Install PyTorch with CUDA**:
   ```bash
   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
   ```
3. **Verify**:
   ```python
   import torch
   print(torch.cuda.is_available())  # Should be True
   ```

### For Railway/Docker:
The Dockerfile should include CUDA base image:
```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04
# ... rest of Dockerfile
```

**Note**: Railway free tier doesn't support GPU. You'll need Railway Pro or other GPU hosting.

---

## Troubleshooting

### "CUDA not available" but I have a GPU
1. **Check CUDA installation**: `nvcc --version`
2. **Check PyTorch CUDA**: `python -c "import torch; print(torch.cuda.is_available())"`
3. **Install PyTorch with CUDA**: Use the index-url above
4. **Check GPU drivers**: `nvidia-smi`

### "FORCE_DEVICE=cuda" but bot uses CPU
- CUDA is not actually available on the system
- Check if GPU is accessible: `nvidia-smi`
- Install CUDA-enabled PyTorch

### Railway shows "CUDA not available"
- Railway free tier doesn't provide GPU
- Railway Pro may have GPU options (check Railway docs)
- Consider GPU hosting alternatives

---

## Expected Log Output

### With CUDA:
```
‚úÖ CUDA detected: Using GPU - NVIDIA GeForce RTX 3080 (10.00 GB)
üöÄ GPU mode: Processing will be 10-50x faster than CPU!
üîÑ Loading detection model on CUDA...
üöÄ GPU: NVIDIA GeForce RTX 3080 (10.00 GB)
```

### Without CUDA:
```
‚ö†Ô∏è CUDA not available: Using CPU (10-50x slower than GPU)
   For maximum speed, use a GPU-enabled server or local machine with CUDA
üîÑ Loading detection model on CPU...
```

---

## Summary

‚úÖ **Bot now automatically uses CUDA if available** - no configuration needed!
‚úÖ **10-50x faster processing** with GPU
‚úÖ **Can force device** via `FORCE_DEVICE` environment variable
‚úÖ **Works on local machine** with NVIDIA GPU
‚ö†Ô∏è **Railway free tier** only has CPU (consider GPU hosting for production)

---

**The bot is now optimized to use CUDA automatically for maximum speed!** üöÄ

